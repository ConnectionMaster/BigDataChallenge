{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_roc_cur(fper, tper, title):  \n",
    "    plt.plot(fper, tper, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC-' + title)\n",
    "    plt.legend()\n",
    "    # plt.savefig(plots_folder + 'roc.png')\n",
    "    plt.show()\n",
    "    \n",
    "# label and label_pred both numpy arrays\n",
    "def evaluate(label_pred, label, label_cols=range(11)):\n",
    "    thresh = 0.5\n",
    "    label_true = label\n",
    "    \n",
    "    # normalize so that pred_0 + pred_1 = 1\n",
    "    for i in range(label_true.shape[1]//2):\n",
    "        label_pred[:,2*i:2*i+2] = tf.nn.softmax(label_pred[:,2*i:2*i+2],axis=1)\n",
    "        \n",
    "    prob = label_pred*label_true\n",
    "    \n",
    "    print(': the overall accuracy is:', sum(sum(prob>thresh))/sum(sum(label>0)))\n",
    "    print(': the overall recall is:', sum(sum(prob>thresh)[1::2])/sum(sum(label>0)[1::2]))\n",
    "    \n",
    "    meas_steps = label_cols\n",
    "    result = pd.DataFrame(index=meas_steps, columns = ['tp','tn','fn','fp','tpr','fpr','min_dis', 'auc'])\n",
    "    for i in range(len(meas_steps)):\n",
    "        neg = prob[:,2*i]\n",
    "        pos = prob[:,2*i+1]\n",
    "\n",
    "        pos = pos[pos!=0]\n",
    "        neg = neg[neg!=0]\n",
    "\n",
    "        tp = sum(pos>thresh)\n",
    "        fn = sum(pos<thresh)\n",
    "        tn = sum(neg>thresh)\n",
    "        fp = sum(neg<thresh)\n",
    "        tpr = tp/(tp+fn+1e-9)\n",
    "        fpr = fp/(fp+tn+1e-9)\n",
    "\n",
    "        y_prob = np.append(pos, neg)\n",
    "        y_true = np.append([1]*len(pos), [0]*len(neg))\n",
    "        if len(pos) and len(neg):\n",
    "            fper, tper, thresholds = roc_curve(np.append([1]*len(pos), [0]*len(neg)),np.append(pos, 1-neg))\n",
    "            auc = roc_auc_score(np.append([1]*len(pos), [0]*len(neg)),np.append(pos, 1-neg))\n",
    "            min_dis = np.sqrt(fper**2 + (1-tper)**2).min()\n",
    "        else:\n",
    "            min_dis = None\n",
    "            auc = None\n",
    "        result.iloc[i] = [tp,tn,fn,fp,tpr,fpr,min_dis,auc]\n",
    "        \n",
    "        plot_roc_cur(fper, tper, str(meas_steps[i]))\n",
    "    \n",
    "    total_sample = sum(sum(result[['tp', 'tn', 'fp', 'fn']].values))\n",
    "    # final score\n",
    "    result['score'] = (result.tp + result.tn + result.fp + result.fn)*result.auc/total_sample\n",
    "    display(result)\n",
    "    print(sum(result.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(label_pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
