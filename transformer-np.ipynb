{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the numpy data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-06-28 18:49:32'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = \"np\"\n",
    "date = datetime.now(pytz.timezone(\"US/Central\")).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_drop_0.5_epoch_1000_batch_2048_bias_1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper parameters\n",
    "reg_kernal, reg_activity = 1e-6,1e-6\n",
    "kernel_regularizer=tf.keras.regularizers.l1(reg_kernal)\n",
    "activity_regularizer=tf.keras.regularizers.l2(reg_activity)\n",
    "#kernel_regularizer=None\n",
    "#activity_regularizer=None\n",
    "  \n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor='val_loss', min_delta=0.0001,\n",
    "  patience=100,restore_best_weights=False)\n",
    "cat_cols = ['tool_name', 'stage', 'step', 'recipe', 'wafer_lottype']\n",
    "local_folder = './'\n",
    "\n",
    "variance_on = True\n",
    "single_meas = False\n",
    "cover2019 = False\n",
    "bias_factor = 1\n",
    "\n",
    "# hyper parameters\n",
    "num_layers = 3\n",
    "\n",
    "dff = 128\n",
    "num_heads = 1\n",
    "dropout_rate = 0.3\n",
    "lr = 1e-4\n",
    "lr_factor = 0.5\n",
    "warmup = 4000\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 2048\n",
    "wafer_col = 'substrate'\n",
    "\n",
    "model_name = 'layer_%s_dff_%s_head_%s_kernel_%s_act_%s_drop_%s_epoch_%s_batch_%s_bias_%s'%(num_layers, \n",
    "                   dff, num_heads, reg_kernal, reg_activity, dropout_rate, epochs, batch_size, bias_factor)\n",
    "if cover2019:\n",
    "    model_name += '_2019'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_set = '602'\n",
    "#grouping = 'mod_name'\n",
    "grouping = 'recipe_step_num'\n",
    "model_name += tool_set + grouping\n",
    "plots_folder = 'plots/' + tool_set + grouping+ '/'\n",
    "results_folder = 'results/'+ tool_set + grouping+ '/'\n",
    "\n",
    "if not os.path.isdir(plots_folder):\n",
    "    os.makedirs(plots_folder)\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deeplearning_data/benchmarking_dataset/sensors_while_processing/602/recipe_step_num/np/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "local_folder = 'challenge_data/'\n",
    "np_path = local_folder\n",
    "ACCESS_KEY = 'AKIAQZBBAFGDBJUIMGAB'\n",
    "SECRET_KEY = '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'\n",
    "BUCKET = 'stx-usw2-ehc-wafer-kpiv-kpov'\n",
    "aws_path = 'Deeplearning_data/benchmarking_dataset/sensors_while_processing/%s/%s/np/'%(tool_set,grouping)\n",
    "aws_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def download_from_aws(bucket, s3_file, local_file):\n",
    "    s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY,\n",
    "                      aws_secret_access_key=SECRET_KEY)\n",
    "\n",
    "\n",
    "    try:\n",
    "        file = s3.download_file(bucket, s3_file, local_file)\n",
    "        print(\"Download Successful\")\n",
    "        return file\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found\")\n",
    "        return False\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return False\n",
    "for fname in ['X_train', 'X_val', 'X_test','label_train','label_val','label_test','label_cols']:\n",
    "    download_from_aws(BUCKET,aws_path + '%s.npy'%(fname),local_folder+'%s.npy'%(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(np_path+'X_train.npy')\n",
    "X_val = np.load(np_path+'X_val.npy')\n",
    "X_test = np.load(np_path+'X_test.npy')\n",
    "label_train = np.load(np_path+'label_train.npy')\n",
    "label_val = np.load(np_path+'label_val.npy')\n",
    "label_test = np.load(np_path+'label_test.npy')\n",
    "label_cols = np.load(np_path+'label_cols.npy',allow_pickle=True)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "label_train = tf.convert_to_tensor(label_train, dtype=tf.float32)\n",
    "label_val = tf.convert_to_tensor(label_val, dtype=tf.float32)\n",
    "label_test = tf.convert_to_tensor(label_test, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245713, 8, 269) (44532, 8, 269) (25351, 8, 269)\n",
      "(245713, 8) (44532, 8) (25351, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(label_train.shape, label_val.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,1:-1,:]\n",
    "X_val = X_val[:,1:-1,:]\n",
    "X_test = X_test[:,1:-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.37487028e+00, 1.27975521e+03, 9.05166952e-01, 4.65365530e+01,\n",
       "       2.88206109e+00, 3.37517857e+02, 1.43980110e-01, 1.04969668e+01])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = np.array([label_train.shape[0]/label_train.shape[1]/label_train.numpy()[:,i].sum() for i in range(label_train.shape[1])])\n",
    "\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.37487028e+00, 1.27975521e+03, 9.05166952e-01, 4.65365530e+01,\n",
       "       2.88206109e+00, 3.37517857e+02, 1.43980110e-01, 1.04969668e+01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight[1::2]*=bias_factor\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(w):\n",
    "    def inner_loss(y,y_pred):\n",
    "        loss = 0\n",
    "        weights = w\n",
    "        \n",
    "        for i in range(len(weights)//2):\n",
    "            \n",
    "            y_i = y[:,2*i:2*i+2]\n",
    "            y_pred_i = y_pred[:,2*i:2*i+2]\n",
    "            w_i = weights[2*i:2*i+2]\n",
    "            #y_pred_i /= tf.reduce_sum(y_pred_i, axis=1, keepdims=True)\n",
    "            y_pred_i = tf.nn.softmax(y_pred_i,axis=1)\n",
    "            loss_category = -tf.reduce_mean(y_i*tf.math.log(tf.clip_by_value(y_pred_i,1e-10,1.0)),axis=0)\n",
    "            loss_i = tf.reduce_sum(loss_category*w_i)\n",
    "            \n",
    "            loss += loss_i\n",
    "\n",
    "        return loss\n",
    "    return inner_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# masking\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq[:,:,-1], 1), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model,activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)\n",
    "        self.wk = tf.keras.layers.Dense(d_model,activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)\n",
    "        self.wv = tf.keras.layers.Dense(d_model,activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model,activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu',\n",
    "                            activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model,\n",
    "                            activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        # x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiLossLayer(Layer):\n",
    "    def __init__(self, nb_outputs, **kwargs):\n",
    "        self.nb_outputs = nb_outputs # nb_outputs = 2*#meas_steps\n",
    "        self.is_placeholder = True\n",
    "        super(CustomMultiLossLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape=None):\n",
    "        # initialise log_vars\n",
    "        self.log_vars = []\n",
    "        for i in range(self.nb_outputs//2):\n",
    "            self.log_vars += [self.add_weight(name='log_var' + str(i), shape=(1,),\n",
    "                                              initializer=Constant(0.), trainable=variance_on)]\n",
    "        super(CustomMultiLossLayer, self).build(input_shape)\n",
    "\n",
    "    def multi_loss(self, y_true, y_pred, weights):\n",
    "        loss = 0        \n",
    "        # cross-entropy part\n",
    "        for i in range(self.nb_outputs//2):\n",
    "            \n",
    "            y_i = y_true[:,2*i:2*i+2]\n",
    "            y_pred_i = y_pred[:,2*i:2*i+2]\n",
    "            w_i = weights[2*i:2*i+2]\n",
    "            #y_pred_i /= tf.reduce_sum(y_pred_i, axis=1, keepdims=True)\n",
    "            y_pred_i = tf.nn.softmax(y_pred_i,axis=1)\n",
    "            loss_category = -tf.reduce_mean(y_i*tf.math.log(tf.clip_by_value(y_pred_i,1e-10,1.0)),axis=0)\n",
    "            loss_category*=w_i\n",
    "            #vw = self.log_vars[2*i:2*i+2]\n",
    "            #p1, p2 = K.exp(-vw[0][0]*2), K.exp(-vw[1][0]*2)\n",
    "            #loss_i = p1*loss_category[0] + vw[0][0] + p2*loss_category[1] + vw[1][0]\n",
    "            \n",
    "            sigma = self.log_vars[i][0]\n",
    "            loss_i = K.exp(-sigma*2)*loss_category[0] + sigma + K.exp(-sigma*2)*loss_category[1] + sigma\n",
    "            # loss_i += (p1+p2)*1e-10\n",
    "            \n",
    "            loss += loss_i\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def call(self, inputs, weights):\n",
    "        y_true = inputs[0]\n",
    "        y_pred = inputs[1]\n",
    "        loss = self.multi_loss(y_true, y_pred, weights)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sst_model(inp_dim, out_dim, weights, num_layers, d_model, num_heads, dff, \n",
    "                input_vocab_size, target_vocab_size, pe_input, rate):\n",
    "    inp = Input(shape=(inp_dim,d_model,))\n",
    "    \n",
    "    tokenizer = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                        input_vocab_size, pe_input, rate)\n",
    "    #final_layer = Dense(target_vocab_size, activation='sigmoid',\n",
    "    #                    activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)\n",
    "    final_layer = tf.keras.Sequential([Dense(dff, activation='sigmoid',\n",
    "                                activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer),\n",
    "                                tf.keras.layers.Dropout(rate),\n",
    "                                Dense(dff, activation='sigmoid'),\n",
    "                                tf.keras.layers.Dropout(rate),\n",
    "                                tf.keras.layers.Dense(target_vocab_size, activation='sigmoid',\n",
    "                                activity_regularizer=activity_regularizer, kernel_regularizer=kernel_regularizer)])\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    enc_output = tokenizer(inp, True, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    dec_output = layers.GlobalAveragePooling1D()(enc_output) # to do: change to flatten (need to support masking)\n",
    "    y_pred = final_layer(dec_output)\n",
    "    \n",
    "    y_true = Input(shape=(out_dim,), name='y_true')\n",
    "    out = CustomMultiLossLayer(nb_outputs=out_dim)([y_true, y_pred],weights)\n",
    "    return Model([inp, y_true], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dim = X_train.shape[1]\n",
    "d_model = X_train.shape[2]\n",
    "#input_vocab_size = d_model\n",
    "#target_vocab_size \n",
    "out_dim= label_train.shape[1]\n",
    "try:\n",
    "    transformer = get_sst_model(\n",
    "        inp_dim = inp_dim,\n",
    "        out_dim = out_dim,\n",
    "        weights = class_weight,\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        input_vocab_size=d_model,\n",
    "        target_vocab_size=out_dim,\n",
    "        pe_input=1000,\n",
    "        rate=dropout_rate)\n",
    "except:\n",
    "    transformer = get_sst_model(\n",
    "        inp_dim = inp_dim,\n",
    "        out_dim = out_dim,\n",
    "        weights = class_weight,\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=1,\n",
    "        dff=dff,\n",
    "        input_vocab_size=d_model,\n",
    "        target_vocab_size=out_dim,\n",
    "        pe_input=1000,\n",
    "        rate=dropout_rate)\n",
    "num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning rate decay\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=warmup):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) * lr_factor\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(100000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.compile(optimizer=optimizer, loss=None)\n",
    "transformer.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "120/120 [==============================] - 7s 55ms/step - loss: 0.8977 - val_loss: 0.7416\n",
      "Epoch 2/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.7723 - val_loss: 0.6356\n",
      "Epoch 3/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.6763 - val_loss: 0.5368\n",
      "Epoch 4/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.5782 - val_loss: 0.4388\n",
      "Epoch 5/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.4776 - val_loss: 0.3443\n",
      "Epoch 6/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.3769 - val_loss: 0.2556\n",
      "Epoch 7/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.2965 - val_loss: 0.1657\n",
      "Epoch 8/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.1938 - val_loss: 0.1032\n",
      "Epoch 9/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.1109 - val_loss: -0.0119\n",
      "Epoch 10/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 0.0261 - val_loss: -0.0933\n",
      "Epoch 11/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.0607 - val_loss: -0.1815\n",
      "Epoch 12/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.1474 - val_loss: -0.2547\n",
      "Epoch 13/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.2282 - val_loss: -0.3196\n",
      "Epoch 14/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.3064 - val_loss: -0.4227\n",
      "Epoch 15/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.3916 - val_loss: -0.5013\n",
      "Epoch 16/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.4678 - val_loss: -0.5778\n",
      "Epoch 17/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.5465 - val_loss: -0.6470\n",
      "Epoch 18/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.6255 - val_loss: -0.6968\n",
      "Epoch 19/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.7011 - val_loss: -0.7917\n",
      "Epoch 20/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.7715 - val_loss: -0.8891\n",
      "Epoch 21/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.8421 - val_loss: -0.9490\n",
      "Epoch 22/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.9214 - val_loss: -1.0307\n",
      "Epoch 23/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -0.9881 - val_loss: -1.0955\n",
      "Epoch 24/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.0682 - val_loss: -1.1423\n",
      "Epoch 25/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.1404 - val_loss: -1.2436\n",
      "Epoch 26/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -1.1847 - val_loss: -1.3175\n",
      "Epoch 27/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.2731 - val_loss: -1.3686\n",
      "Epoch 28/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.3322 - val_loss: -1.4511\n",
      "Epoch 29/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.3966 - val_loss: -1.5079\n",
      "Epoch 30/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.4713 - val_loss: -1.5755\n",
      "Epoch 31/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.5225 - val_loss: -1.6603\n",
      "Epoch 32/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.6027 - val_loss: -1.6992\n",
      "Epoch 33/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.6626 - val_loss: -1.7847\n",
      "Epoch 34/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.7343 - val_loss: -1.8606\n",
      "Epoch 35/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.7944 - val_loss: -1.8811\n",
      "Epoch 36/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.8427 - val_loss: -1.9589\n",
      "Epoch 37/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.9092 - val_loss: -2.0488\n",
      "Epoch 38/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -1.9607 - val_loss: -2.1062\n",
      "Epoch 39/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.0202 - val_loss: -2.1728\n",
      "Epoch 40/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.0638 - val_loss: -2.1988\n",
      "Epoch 41/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.1393 - val_loss: -2.2779\n",
      "Epoch 42/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.2001 - val_loss: -2.3031\n",
      "Epoch 43/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.2566 - val_loss: -2.3964\n",
      "Epoch 44/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.3103 - val_loss: -2.4322\n",
      "Epoch 45/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.3604 - val_loss: -2.5042\n",
      "Epoch 46/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.4226 - val_loss: -2.5513\n",
      "Epoch 47/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.4504 - val_loss: -2.4845\n",
      "Epoch 48/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.5230 - val_loss: -2.5695\n",
      "Epoch 49/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.5593 - val_loss: -2.6434\n",
      "Epoch 50/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.5928 - val_loss: -2.7544\n",
      "Epoch 51/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.6608 - val_loss: -2.7431\n",
      "Epoch 52/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.6970 - val_loss: -2.8545\n",
      "Epoch 53/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.7436 - val_loss: -2.8416\n",
      "Epoch 54/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.8064 - val_loss: -2.8941\n",
      "Epoch 55/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.8356 - val_loss: -2.8353\n",
      "Epoch 56/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.8754 - val_loss: -2.9810\n",
      "Epoch 57/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.9081 - val_loss: -3.1082\n",
      "Epoch 58/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -2.9765 - val_loss: -3.0273\n",
      "Epoch 59/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.0247 - val_loss: -3.2026\n",
      "Epoch 60/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.0166 - val_loss: -3.0034\n",
      "Epoch 61/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.1081 - val_loss: -3.0685\n",
      "Epoch 62/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.0723 - val_loss: -3.3060\n",
      "Epoch 63/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.1493 - val_loss: -3.3115\n",
      "Epoch 64/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.1659 - val_loss: -3.2458\n",
      "Epoch 65/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.2088 - val_loss: -3.3280\n",
      "Epoch 66/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.2441 - val_loss: -3.3309\n",
      "Epoch 67/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.2630 - val_loss: -3.4009\n",
      "Epoch 68/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.2955 - val_loss: -3.5039\n",
      "Epoch 69/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.3134 - val_loss: -3.2758\n",
      "Epoch 70/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.3587 - val_loss: -3.5793\n",
      "Epoch 71/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.3842 - val_loss: -3.4805\n",
      "Epoch 72/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.3891 - val_loss: -3.5611\n",
      "Epoch 73/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.4165 - val_loss: -3.6726\n",
      "Epoch 74/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.4582 - val_loss: -3.6928\n",
      "Epoch 75/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.4879 - val_loss: -3.6899\n",
      "Epoch 76/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.5005 - val_loss: -3.7214\n",
      "Epoch 77/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.5495 - val_loss: -3.5149\n",
      "Epoch 78/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.5251 - val_loss: -3.6686\n",
      "Epoch 79/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.5695 - val_loss: -3.8147\n",
      "Epoch 80/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.5955 - val_loss: -3.7299\n",
      "Epoch 81/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.5733 - val_loss: -3.8839\n",
      "Epoch 82/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6212 - val_loss: -3.5537\n",
      "Epoch 83/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.5816 - val_loss: -3.7665\n",
      "Epoch 84/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6560 - val_loss: -3.7590\n",
      "Epoch 85/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6570 - val_loss: -3.9730\n",
      "Epoch 86/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6465 - val_loss: -3.7802\n",
      "Epoch 87/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6467 - val_loss: -3.8675\n",
      "Epoch 88/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7177 - val_loss: -3.9864\n",
      "Epoch 89/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6992 - val_loss: -3.9954\n",
      "Epoch 90/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7362 - val_loss: -3.8383\n",
      "Epoch 91/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7451 - val_loss: -3.8479\n",
      "Epoch 92/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6797 - val_loss: -3.9077\n",
      "Epoch 93/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6818 - val_loss: -4.0187\n",
      "Epoch 94/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7135 - val_loss: -3.8887\n",
      "Epoch 95/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7703 - val_loss: -3.7083\n",
      "Epoch 96/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7037 - val_loss: -3.9058\n",
      "Epoch 97/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7698 - val_loss: -4.0921\n",
      "Epoch 98/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7822 - val_loss: -3.8752\n",
      "Epoch 99/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7795 - val_loss: -3.7320\n",
      "Epoch 100/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7672 - val_loss: -3.9403\n",
      "Epoch 101/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7756 - val_loss: -3.9050\n",
      "Epoch 102/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7907 - val_loss: -4.1270\n",
      "Epoch 103/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7367 - val_loss: -4.0248\n",
      "Epoch 104/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8116 - val_loss: -4.1730\n",
      "Epoch 105/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8101 - val_loss: -4.1281\n",
      "Epoch 106/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7967 - val_loss: -3.9798\n",
      "Epoch 107/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7325 - val_loss: -3.7258\n",
      "Epoch 108/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8314 - val_loss: -4.0141\n",
      "Epoch 109/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8260 - val_loss: -3.7526\n",
      "Epoch 110/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8053 - val_loss: -4.0371\n",
      "Epoch 111/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8661 - val_loss: -4.0470\n",
      "Epoch 112/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7973 - val_loss: -3.8175\n",
      "Epoch 113/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8163 - val_loss: -3.9575\n",
      "Epoch 114/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8110 - val_loss: -4.0718\n",
      "Epoch 115/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8354 - val_loss: -4.1430\n",
      "Epoch 116/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8443 - val_loss: -3.7987\n",
      "Epoch 117/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8779 - val_loss: -3.7863\n",
      "Epoch 118/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7688 - val_loss: -4.0202\n",
      "Epoch 119/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8520 - val_loss: -3.9482\n",
      "Epoch 120/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8223 - val_loss: -3.9820\n",
      "Epoch 121/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8327 - val_loss: -3.8039\n",
      "Epoch 122/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.6937 - val_loss: -3.7857\n",
      "Epoch 123/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8646 - val_loss: -3.8991\n",
      "Epoch 124/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8645 - val_loss: -4.2243\n",
      "Epoch 125/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8679 - val_loss: -4.0618\n",
      "Epoch 126/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8248 - val_loss: -4.0223\n",
      "Epoch 127/1000\n",
      "120/120 [==============================] - 5s 43ms/step - loss: -3.7913 - val_loss: -4.2597\n",
      "Epoch 128/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8248 - val_loss: -3.7835\n",
      "Epoch 129/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8616 - val_loss: -4.1954\n",
      "Epoch 130/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8301 - val_loss: -4.0375\n",
      "Epoch 131/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8718 - val_loss: -4.0369\n",
      "Epoch 132/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8566 - val_loss: -4.1681\n",
      "Epoch 133/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8580 - val_loss: -4.2463\n",
      "Epoch 134/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9155 - val_loss: -3.7761\n",
      "Epoch 135/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8509 - val_loss: -4.2077\n",
      "Epoch 136/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8682 - val_loss: -4.2067\n",
      "Epoch 137/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8007 - val_loss: -4.2221\n",
      "Epoch 138/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8362 - val_loss: -4.2434\n",
      "Epoch 139/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8994 - val_loss: -4.2162\n",
      "Epoch 140/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.7992 - val_loss: -4.0011\n",
      "Epoch 141/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8606 - val_loss: -4.2559\n",
      "Epoch 142/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8830 - val_loss: -3.8163\n",
      "Epoch 143/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8855 - val_loss: -4.2323\n",
      "Epoch 144/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9084 - val_loss: -3.8685\n",
      "Epoch 145/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9256 - val_loss: -4.0927\n",
      "Epoch 146/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8662 - val_loss: -4.1022\n",
      "Epoch 147/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8885 - val_loss: -4.2329\n",
      "Epoch 148/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8977 - val_loss: -4.0258\n",
      "Epoch 149/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8654 - val_loss: -3.7793\n",
      "Epoch 150/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9118 - val_loss: -4.0585\n",
      "Epoch 151/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8453 - val_loss: -4.1385\n",
      "Epoch 152/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8007 - val_loss: -4.2722\n",
      "Epoch 153/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9239 - val_loss: -4.1940\n",
      "Epoch 154/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9083 - val_loss: -4.0489\n",
      "Epoch 155/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8714 - val_loss: -4.0639\n",
      "Epoch 156/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9031 - val_loss: -4.1894\n",
      "Epoch 157/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8787 - val_loss: -4.2827\n",
      "Epoch 158/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8648 - val_loss: -4.2084\n",
      "Epoch 159/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8543 - val_loss: -4.0684\n",
      "Epoch 160/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.9108 - val_loss: -4.0605\n",
      "Epoch 161/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.9491 - val_loss: -4.0939\n",
      "Epoch 162/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9168 - val_loss: -4.0126\n",
      "Epoch 163/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9464 - val_loss: -4.0870\n",
      "Epoch 164/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8858 - val_loss: -4.0887\n",
      "Epoch 165/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9538 - val_loss: -4.0516\n",
      "Epoch 166/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8719 - val_loss: -4.0163\n",
      "Epoch 167/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8894 - val_loss: -4.0903\n",
      "Epoch 168/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8913 - val_loss: -3.8119\n",
      "Epoch 169/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9184 - val_loss: -3.8028\n",
      "Epoch 170/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9056 - val_loss: -3.8637\n",
      "Epoch 171/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8966 - val_loss: -3.8468\n",
      "Epoch 172/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8004 - val_loss: -4.3913\n",
      "Epoch 173/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.7065 - val_loss: -4.0706\n",
      "Epoch 174/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8012 - val_loss: -4.3131\n",
      "Epoch 175/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8918 - val_loss: -4.0955\n",
      "Epoch 176/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8709 - val_loss: -4.3152\n",
      "Epoch 177/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8947 - val_loss: -3.7697\n",
      "Epoch 178/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8995 - val_loss: -4.0345\n",
      "Epoch 179/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9205 - val_loss: -4.0160\n",
      "Epoch 180/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9293 - val_loss: -4.3209\n",
      "Epoch 181/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8564 - val_loss: -4.2983\n",
      "Epoch 182/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8949 - val_loss: -4.0364\n",
      "Epoch 183/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9091 - val_loss: -3.8005\n",
      "Epoch 184/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9210 - val_loss: -4.0252\n",
      "Epoch 185/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8805 - val_loss: -3.9798\n",
      "Epoch 186/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8728 - val_loss: -3.7864\n",
      "Epoch 187/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9604 - val_loss: -3.7985\n",
      "Epoch 188/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8192 - val_loss: -3.9093\n",
      "Epoch 189/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9438 - val_loss: -4.0454\n",
      "Epoch 190/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9264 - val_loss: -3.9569\n",
      "Epoch 191/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8773 - val_loss: -4.2576\n",
      "Epoch 192/1000\n",
      "120/120 [==============================] - 5s 44ms/step - loss: -3.8929 - val_loss: -4.0750\n",
      "Epoch 193/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9242 - val_loss: -4.0107\n",
      "Epoch 194/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9394 - val_loss: -3.7500\n",
      "Epoch 195/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9432 - val_loss: -4.0305\n",
      "Epoch 196/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8738 - val_loss: -3.7349\n",
      "Epoch 197/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9012 - val_loss: -3.8255\n",
      "Epoch 198/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9188 - val_loss: -4.0876\n",
      "Epoch 199/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9610 - val_loss: -4.3616\n",
      "Epoch 200/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8988 - val_loss: -4.0919\n",
      "Epoch 201/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8660 - val_loss: -4.0172\n",
      "Epoch 202/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8968 - val_loss: -4.3129\n",
      "Epoch 203/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9142 - val_loss: -4.2606\n",
      "Epoch 204/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8775 - val_loss: -4.0107\n",
      "Epoch 205/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8924 - val_loss: -4.0546\n",
      "Epoch 206/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8998 - val_loss: -4.3202\n",
      "Epoch 207/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9267 - val_loss: -4.3375\n",
      "Epoch 208/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.9185 - val_loss: -3.8200\n",
      "Epoch 209/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9404 - val_loss: -4.3214\n",
      "Epoch 210/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9177 - val_loss: -4.0915\n",
      "Epoch 211/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8050 - val_loss: -4.2347\n",
      "Epoch 212/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8643 - val_loss: -4.1612\n",
      "Epoch 213/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9596 - val_loss: -3.8030\n",
      "Epoch 214/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8891 - val_loss: -4.2107\n",
      "Epoch 215/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9211 - val_loss: -4.0713\n",
      "Epoch 216/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9320 - val_loss: -4.3089\n",
      "Epoch 217/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9371 - val_loss: -4.0797\n",
      "Epoch 218/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8769 - val_loss: -4.0993\n",
      "Epoch 219/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9040 - val_loss: -4.0112\n",
      "Epoch 220/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8835 - val_loss: -4.3051\n",
      "Epoch 221/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9446 - val_loss: -3.7807\n",
      "Epoch 222/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9463 - val_loss: -4.1169\n",
      "Epoch 223/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9370 - val_loss: -4.1161\n",
      "Epoch 224/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9454 - val_loss: -3.8452\n",
      "Epoch 225/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9460 - val_loss: -4.0983\n",
      "Epoch 226/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9054 - val_loss: -4.0339\n",
      "Epoch 227/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9125 - val_loss: -4.2210\n",
      "Epoch 228/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9141 - val_loss: -4.0653\n",
      "Epoch 229/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8887 - val_loss: -4.0389\n",
      "Epoch 230/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.9492 - val_loss: -3.9249\n",
      "Epoch 231/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9671 - val_loss: -4.0992\n",
      "Epoch 232/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9113 - val_loss: -3.8407\n",
      "Epoch 233/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8008 - val_loss: -4.0844\n",
      "Epoch 234/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9375 - val_loss: -4.3234\n",
      "Epoch 235/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9540 - val_loss: -4.0781\n",
      "Epoch 236/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9752 - val_loss: -4.2215\n",
      "Epoch 237/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9431 - val_loss: -3.7029\n",
      "Epoch 238/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8935 - val_loss: -4.2534\n",
      "Epoch 239/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9319 - val_loss: -4.0776\n",
      "Epoch 240/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9016 - val_loss: -4.3636\n",
      "Epoch 241/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9720 - val_loss: -4.1477\n",
      "Epoch 242/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8627 - val_loss: -4.1190\n",
      "Epoch 243/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9046 - val_loss: -4.3551\n",
      "Epoch 244/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9517 - val_loss: -4.0690\n",
      "Epoch 245/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9667 - val_loss: -4.3529\n",
      "Epoch 246/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8663 - val_loss: -3.8662\n",
      "Epoch 247/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8133 - val_loss: -4.0722\n",
      "Epoch 248/1000\n",
      "120/120 [==============================] - 5s 41ms/step - loss: -3.8826 - val_loss: -4.2135\n",
      "Epoch 249/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8942 - val_loss: -4.0225\n",
      "Epoch 250/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9780 - val_loss: -4.1005\n",
      "Epoch 251/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9547 - val_loss: -3.8761\n",
      "Epoch 252/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8903 - val_loss: -3.8229\n",
      "Epoch 253/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9026 - val_loss: -3.9778\n",
      "Epoch 254/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8846 - val_loss: -4.3175\n",
      "Epoch 255/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9902 - val_loss: -4.1189\n",
      "Epoch 256/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8901 - val_loss: -3.9943\n",
      "Epoch 257/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9781 - val_loss: -4.0810\n",
      "Epoch 258/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9256 - val_loss: -4.1060\n",
      "Epoch 259/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9135 - val_loss: -4.0577\n",
      "Epoch 260/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9619 - val_loss: -4.1296\n",
      "Epoch 261/1000\n",
      "120/120 [==============================] - 5s 43ms/step - loss: -3.9884 - val_loss: -3.8005\n",
      "Epoch 262/1000\n",
      "120/120 [==============================] - 5s 43ms/step - loss: -3.8899 - val_loss: -4.0747\n",
      "Epoch 263/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9499 - val_loss: -4.0880\n",
      "Epoch 264/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9836 - val_loss: -4.0950\n",
      "Epoch 265/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9233 - val_loss: -4.3807\n",
      "Epoch 266/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9784 - val_loss: -4.0924\n",
      "Epoch 267/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9398 - val_loss: -3.8514\n",
      "Epoch 268/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9782 - val_loss: -3.8770\n",
      "Epoch 269/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.8824 - val_loss: -4.3544\n",
      "Epoch 270/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9642 - val_loss: -4.0392\n",
      "Epoch 271/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9736 - val_loss: -4.0956\n",
      "Epoch 272/1000\n",
      "120/120 [==============================] - 5s 42ms/step - loss: -3.9188 - val_loss: -3.9986\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 17s, sys: 3min 39s, total: 37min 57s\n",
      "Wall time: 23min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist = transformer.fit([X_train, label_train],label_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size = batch_size, \n",
    "                    verbose=1,\n",
    "                    callbacks=[earlystop_callback],\n",
    "                    validation_data=([X_val,label_val], label_val))\n",
    "\n",
    "# plot the loss\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='val')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.yscale('log')\n",
    "plt.title('transformer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [3.2340705, 2.7659972, 2.412408, 2.595242]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6, 269)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 6)]          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Equal (TensorFlowOp [(None, 6)]          0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cast (TensorFlowOpL [(None, 6)]          0           tf_op_layer_Equal[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 1, 1, 6)]    0           tf_op_layer_Cast[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Encoder)               (None, 6, 269)       1082571     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 269)          0           encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "y_true (InputLayer)             [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 8)            52104       global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "custom_multi_loss_layer (Custom (None, 8)            4           y_true[0][0]                     \n",
      "                                                                 sequential_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,134,679\n",
      "Trainable params: 1,134,679\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('weights:', [np.exp(-K.get_value(log_var[0])) for log_var in transformer.layers[-1].log_vars])\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082571 2160\n"
     ]
    }
   ],
   "source": [
    "# parameter check\n",
    "\n",
    "d = d_model\n",
    "n_p = num_layers*(4*(num_heads*d*d/num_heads + d) + 2*d * 2 + (d+1)*dff + (dff+1)*d)\n",
    "n_cls = (d+1)*len(label_cols)\n",
    "print(int(n_p),n_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_roc_cur(fper, tper, title):  \n",
    "    plt.plot(fper, tper, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC-' + title)\n",
    "    plt.legend()\n",
    "    # plt.savefig(plots_folder + 'roc.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def evaluate(classifier, X, label, label_cols, model_name, split_name, plot=False, save=True):\n",
    "    thresh = 0.5\n",
    "    label_pred = transformer.predict([X,label])\n",
    "    label_true = label\n",
    "    for i in range(label_true.shape[1]//2):\n",
    "\n",
    "        label_pred[:,2*i:2*i+2] = tf.nn.softmax(label_pred[:,2*i:2*i+2],axis=1)\n",
    "        \n",
    "    \n",
    "    prob = label_pred*label_true\n",
    "    \n",
    "    print(split_name, ': the overall accuracy is:', sum(sum(prob.numpy()>thresh))/sum(sum(label.numpy()>0)))\n",
    "    print(split_name, ': the overall recall is:', sum(sum(prob.numpy()>thresh)[1::2])/sum(sum(label.numpy()>0)[1::2]))\n",
    "    \n",
    "    meas_steps = [ms.split(':')[0] for ms in label_cols][::2]\n",
    "    result = pd.DataFrame(index=meas_steps, columns = ['model', 'split', 'tp','tn','fn','fp','tpr','fpr','min_dis', 'auc', 'data_path'])\n",
    "    for i in range(len(meas_steps)):\n",
    "        neg = prob[:,2*i]\n",
    "        pos = prob[:,2*i+1]\n",
    "\n",
    "        pos = pos[pos!=0].numpy()\n",
    "        neg = neg[neg!=0].numpy()\n",
    "\n",
    "        tp = sum(pos>thresh)\n",
    "        fn = sum(pos<thresh)\n",
    "        tn = sum(neg>thresh)\n",
    "        fp = sum(neg<thresh)\n",
    "        tpr = tp/(tp+fn+1e-9)\n",
    "        fpr = fp/(fp+tn+1e-9)\n",
    "\n",
    "        y_prob = np.append(pos, neg)\n",
    "        y_true = np.append([1]*len(pos), [0]*len(neg))\n",
    "        if len(pos) and len(neg):\n",
    "            fper, tper, thresholds = roc_curve(np.append([1]*len(pos), [0]*len(neg)),np.append(pos, 1-neg))\n",
    "            auc = roc_auc_score(np.append([1]*len(pos), [0]*len(neg)),np.append(pos, 1-neg))\n",
    "            min_dis = np.sqrt(fper**2 + (1-tper)**2).min()\n",
    "        else:\n",
    "            min_dis = None\n",
    "            auc = None\n",
    "        result.iloc[i] = [model_name,split_name,tp,tn,fn,fp,tpr,fpr,min_dis,auc,aws_path]\n",
    "        if plot:\n",
    "            plot_roc_cur(fper, tper, meas_steps[i])\n",
    "    if save:\n",
    "        result.to_csv(results_folder + 'result_np.csv', mode='a')\n",
    "        print('saved to', results_folder)\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : the overall accuracy is: 0.7069586758247945\n",
      "train : the overall recall is: 0.7060253985409349\n",
      "saved to results/602recipe_step_num/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>min_dis</th>\n",
       "      <th>auc</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAS2_SURFSCAN</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>train</td>\n",
       "      <td>23</td>\n",
       "      <td>4411</td>\n",
       "      <td>1</td>\n",
       "      <td>407</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.0844749</td>\n",
       "      <td>0.0742329</td>\n",
       "      <td>0.975841</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS2_THCK</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>train</td>\n",
       "      <td>520</td>\n",
       "      <td>25606</td>\n",
       "      <td>140</td>\n",
       "      <td>8326</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.245373</td>\n",
       "      <td>0.310759</td>\n",
       "      <td>0.845947</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_SURFSCAN</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>train</td>\n",
       "      <td>76</td>\n",
       "      <td>2024</td>\n",
       "      <td>15</td>\n",
       "      <td>8633</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.810078</td>\n",
       "      <td>0.627586</td>\n",
       "      <td>0.54071</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_THCK</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>train</td>\n",
       "      <td>1994</td>\n",
       "      <td>153701</td>\n",
       "      <td>932</td>\n",
       "      <td>59621</td>\n",
       "      <td>0.681476</td>\n",
       "      <td>0.279488</td>\n",
       "      <td>0.420621</td>\n",
       "      <td>0.763299</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            model  split  \\\n",
       "MEAS2_SURFSCAN  layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  train   \n",
       "MEAS2_THCK      layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  train   \n",
       "MEAS_SURFSCAN   layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  train   \n",
       "MEAS_THCK       layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  train   \n",
       "\n",
       "                  tp      tn   fn     fp       tpr        fpr    min_dis  \\\n",
       "MEAS2_SURFSCAN    23    4411    1    407  0.958333  0.0844749  0.0742329   \n",
       "MEAS2_THCK       520   25606  140   8326  0.787879   0.245373   0.310759   \n",
       "MEAS_SURFSCAN     76    2024   15   8633  0.835165   0.810078   0.627586   \n",
       "MEAS_THCK       1994  153701  932  59621  0.681476   0.279488   0.420621   \n",
       "\n",
       "                     auc                                          data_path  \n",
       "MEAS2_SURFSCAN  0.975841  Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS2_THCK      0.845947  Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS_SURFSCAN    0.54071  Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS_THCK       0.763299  Deeplearning_data/benchmarking_dataset/sensors...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 8.37 s, total: 1min 42s\n",
      "Wall time: 50.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(transformer, X_train, label_train, label_cols, model_name, 'train', plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : the overall accuracy is: 0.7607742042784973\n",
      "val : the overall recall is: 0.5794183445190156\n",
      "saved to results/602recipe_step_num/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>min_dis</th>\n",
       "      <th>auc</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAS2_SURFSCAN</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>val</td>\n",
       "      <td>0</td>\n",
       "      <td>698</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0592992</td>\n",
       "      <td>0.117251</td>\n",
       "      <td>0.902965</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS2_THCK</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>val</td>\n",
       "      <td>19</td>\n",
       "      <td>4754</td>\n",
       "      <td>14</td>\n",
       "      <td>1272</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.211085</td>\n",
       "      <td>0.370021</td>\n",
       "      <td>0.762423</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_SURFSCAN</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>val</td>\n",
       "      <td>24</td>\n",
       "      <td>274</td>\n",
       "      <td>4</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.840047</td>\n",
       "      <td>0.680202</td>\n",
       "      <td>0.528209</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_THCK</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>val</td>\n",
       "      <td>216</td>\n",
       "      <td>30609</td>\n",
       "      <td>168</td>\n",
       "      <td>8564</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.21862</td>\n",
       "      <td>0.471882</td>\n",
       "      <td>0.725059</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            model split   tp  \\\n",
       "MEAS2_SURFSCAN  layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...   val    0   \n",
       "MEAS2_THCK      layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...   val   19   \n",
       "MEAS_SURFSCAN   layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...   val   24   \n",
       "MEAS_THCK       layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...   val  216   \n",
       "\n",
       "                   tn   fn    fp       tpr        fpr   min_dis       auc  \\\n",
       "MEAS2_SURFSCAN    698    2    44         0  0.0592992  0.117251  0.902965   \n",
       "MEAS2_THCK       4754   14  1272  0.575758   0.211085  0.370021  0.762423   \n",
       "MEAS_SURFSCAN     274    4  1439  0.857143   0.840047  0.680202  0.528209   \n",
       "MEAS_THCK       30609  168  8564    0.5625    0.21862  0.471882  0.725059   \n",
       "\n",
       "                                                        data_path  \n",
       "MEAS2_SURFSCAN  Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS2_THCK      Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS_SURFSCAN   Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS_THCK       Deeplearning_data/benchmarking_dataset/sensors...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(transformer, X_val, label_val, label_cols, model_name, 'val', plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : the overall accuracy is: 0.7665858182336995\n",
      "test : the overall recall is: 0.5029585798816568\n",
      "saved to results/602recipe_step_num/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tpr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>min_dis</th>\n",
       "      <th>auc</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAS2_SURFSCAN</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0138568</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS2_THCK</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>test</td>\n",
       "      <td>9</td>\n",
       "      <td>2438</td>\n",
       "      <td>13</td>\n",
       "      <td>699</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.222824</td>\n",
       "      <td>0.457937</td>\n",
       "      <td>0.624496</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_SURFSCAN</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>test</td>\n",
       "      <td>13</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>1088</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.864178</td>\n",
       "      <td>0.726003</td>\n",
       "      <td>0.409584</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAS_THCK</th>\n",
       "      <td>layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...</td>\n",
       "      <td>test</td>\n",
       "      <td>148</td>\n",
       "      <td>17859</td>\n",
       "      <td>153</td>\n",
       "      <td>4577</td>\n",
       "      <td>0.491694</td>\n",
       "      <td>0.204002</td>\n",
       "      <td>0.514488</td>\n",
       "      <td>0.681384</td>\n",
       "      <td>Deeplearning_data/benchmarking_dataset/sensors...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            model split   tp  \\\n",
       "MEAS2_SURFSCAN  layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  test    0   \n",
       "MEAS2_THCK      layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  test    9   \n",
       "MEAS_SURFSCAN   layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  test   13   \n",
       "MEAS_THCK       layer_3_dff_128_head_1_kernel_1e-06_act_1e-06_...  test  148   \n",
       "\n",
       "                   tn   fn    fp       tpr        fpr   min_dis       auc  \\\n",
       "MEAS2_SURFSCAN    854    0    12         0  0.0138568      None      None   \n",
       "MEAS2_THCK       2438   13   699  0.409091   0.222824  0.457937  0.624496   \n",
       "MEAS_SURFSCAN     171    2  1088  0.866667   0.864178  0.726003  0.409584   \n",
       "MEAS_THCK       17859  153  4577  0.491694   0.204002  0.514488  0.681384   \n",
       "\n",
       "                                                        data_path  \n",
       "MEAS2_SURFSCAN  Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS2_THCK      Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS_SURFSCAN   Deeplearning_data/benchmarking_dataset/sensors...  \n",
       "MEAS_THCK       Deeplearning_data/benchmarking_dataset/sensors...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(transformer, X_test, label_test, label_cols, model_name, 'test', plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.2",
   "language": "python",
   "name": "tf2.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
